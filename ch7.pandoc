Natural Deduction Proofs in SL {#ch.ND.proofs}
==============================

This chapter introduces a different proof system in SL, separate from
the tree method. The tree method has advantages and disadvantages. One
advantage of trees is that, for the most part, they can be produced in a
purely mechanical way; another is that, when a tree remains open, the
tree method gives us a recipe for constructing an interpretation that
satisfies the root. One disadvantage is that they do not always
emphasize in an intuitive way why a conclusion follows from a set of
premises; they show that something *must* be the case, on pain of
contradiction, but they don't always demonstrate, in a way closely
connected to natural reasoning, why some things follow from other
things.

The system of this chapter will differ from the tree method in all of
these respects. It is intended to model human reasoning in a closer way,
illustrating the connections between various claims; consequently,
working through a natural deduction proof requires a bit more insight
and inspiration than a tree proof does.

Natural deduction proofs can be used to prove that an argument is valid;
if an argument is invalid, our natural deduction system will not
necessarily make that obvious. (Any valid argument in SL can be shown to
be valid via a natural deduction proof --- i.e., this method is complete
too (and sound) --- but that's no guarantee that any individual will be
able to come up with the proof.)

Consider two arguments in SL:

2 Argument A

$P \eor Q$

$\enot P$

Q

Argument B

$P \eif Q$

$P$

Q

Clearly, these are valid arguments. You can confirm that they are valid
by constructing four-line truth tables, or by using trees. Argument A
makes use of an inference form that is always valid: Given a disjunction
and the negation of one of the disjuncts, the other disjunct follows as
a valid consequence. This rule is called *disjunctive syllogism*.

Argument B makes use of a different valid form: Given a conditional and
its antecedent, the consequent follows as a valid consequence. This is
called *modus ponens*.

When we construct truth tables, we do not need to give names to
different inference forms. There is no reason to distinguish modus
ponens from a disjunctive syllogism. For this same reason, however, the
method of truth tables does not clearly show *why* an argument is valid.
If you were to do a 1024-line truth table for an argument that contains
ten sentence letters, then you could check to see if there were any
lines on which the premises were all true and the conclusion were false.
If you did not see such a line and provided you made no mistakes in
constructing the table, then you would know that the argument was valid.
Yet you would not be able to say anything further about why this
particular argument was a valid argument form.

We aim to show that particular arguments are valid in a way that allows
us to understand the reasoning involved in the argument. We begin with
basic argument forms, like disjunctive syllogism and modus ponens. These
forms can then be combined to make more complicated arguments, like this
one:

$\enot L \eif (J \eor L)$

$\enot L$

$J$

By modus ponens, (1) and (2) entail $J \eor L$. This is an *intermediate
conclusion*. It follows logically from the premises, but it is not the
conclusion we want. Now $J \eor L$ and (2) entail $J$, by disjunctive
syllogism. We do not need a new rule for this argument. The proof of the
argument shows that it is really just a combination of rules we have
already introduced.

Formally, a is a sequence of sentences. The first sentences of the
sequence are assumptions; these are the premises of the argument. Every
sentence later in the sequence follows from earlier sentences by one of
the rules of proof. The final sentence of the sequence is the conclusion
of the argument.

Basic rules for SL
------------------

In designing a proof system, we could just start with disjunctive
syllogism and modus ponens. Whenever we discovered a valid argument
which could not be proven with rules we already had, we could introduce
new rules. Proceeding in this way, we would have an unsystematic grab
bag of rules. We might accidentally add some strange rules, and we would
surely end up with more rules than we need.

Instead, we will develop what is called a system. In a natural deduction
system, there will be two rules for each logical operator: an rule that
allows us to prove a sentence that has it as the main logical operator
and an rule that allows us to prove something given a sentence that has
it as the main logical operator.

In addition to the rules for each logical operator, we will also have a
reiteration rule. If you already have shown something in the course of a
proof, the reiteration rule allows you to repeat it on a new line. For
instance:

The numbers on the left indicate line numbers; they are used for
reference in justifying later steps in the proof. New lines in a proof
must always be justified by rules and reference to previous lines. The
'R 1' to the right on line 2 is the justification for that line --- that
line is permitted by the reiteration rule (R), applied to line 1.

Obviously, the reiteration rule will not allow us to show anything
*new*. For that, we will need more rules. The remainder of this section
will give introduction and elimination rules for all of the sentential
connectives. This will give us a complete proof system for SL.

All of the rules introduced in this chapter are summarized in the Quick
Reference guide at the end of this book.

### Conjunction

What would you need to show in order to prove $E \eand F$? The natural
answer is, you'd need to prove $E$, and you'd also need to prove $F$. In
fact this holds much more generally; one can prove any conjunction by
proving and also proving , whether or not these conjuncts are atomic
sentences. If you can prove $[(A \eor J) \eif V]$ and
$[(V \eif L) \eiff (F \eor N)]$, then you have effectively proven
$$[(A \eor J) \eif V] \eand [(V \eif L) \eiff (F \eor N)].$$ So this
will be our conjunction introduction rule, which we abbreviate 'I':

As always, the and stand in for any arbitrary sentence in SL; the $m$
and $n$ here stand in for arbitrary line numbers. In an actual proof,
the lines are numbered $1, 2, 3, \ldots$ and rules must be applied to
specific line numbers. When we define the rule, however, we use
variables to underscore the point that the rule may be applied to any
two lines that are already in the proof. Note that it is not a
requirement that $m$ and $n$ be consecutive lines, or that they appear
in the order listed here. We require only that each line has been
established somewhere above in the proof. If you have $K$ on line 15 and
$L$ on line 8, you can prove $(K\eand L)$ at some later point in the
proof with the justification 'I 15, 8.' (By convention, we won't always
worry about which order to write the line numbers down in. 'I 8, 15' is
OK too.)

Now, consider the elimination rule for conjunction. What are you
entitled to conclude from a sentence like $E \eand F$? Surely, you are
entitled to conclude $E$; if $E \eand F$ were true, then $E$ would be
true. Similarly, you are entitled to conclude $F$. This will be our
conjunction elimination rule, which we abbreviate 'E':

æab æab

When you have a conjunction on some line of a proof, you can use E to
derive either of the conjuncts. This rule allows either of the two
developments listed here. You may also apply the rule twice, to get
both. The E rule requires only one sentence, so we write one line number
as the justification for applying it.

Even with just these two rules, we can provide some proofs. Consider
this argument.

$[(A\eor B)\eif(C\eor D)] \eand [(E \eor F) \eif (G\eor H)]$

$[(E \eor F) \eif (G\eor H)] \eand [(A\eor B)\eif(C\eor D)]$

The main logical operator in both the premise and conclusion is
conjunction. Since conjunction is symmetric, the argument is obviously
valid. In order to provide a proof, we begin by writing down the
premise. After the premises, we draw a horizontal line --- everything
below this line must be justified by a rule of proof. So the beginning
of the proof looks like this:

From the premise, we can get each of the conjuncts by E. The proof now
looks like this:

æab æab

The rule I requires that we have each of the conjuncts available
somewhere in the proof. They can be separated from one another, and they
can appear in any order. So by applying the I rule to lines 3 and 2, we
arrive at the desired conclusion. The finished proof looks like this:

æab æab

This proof is not terribly interesting, but it shows how we can use
rules of proof together to demonstrate the validity of an argument form.
Note also that using a truth table to show that this argument is valid
would have required a staggering 256 lines, since there are eight
sentence letters in the argument. A proof via trees would be less
unwieldy than that, but it would be less simple and elegant than this
one. (Constructing such a proof would be a good exercise for tree
review.)

### Disjunction

If $M$ is true, then $M \eor N$ must also be true. In general, the
disjunction introduction rule (I) allows us to derive a disjunction if
we have one of the two disjuncts:

Notice that can be *any* sentence whatsoever. So the following is a
legitimate proof:

It may seem odd that just by knowing $M$ we can derive a conclusion that
includes sentences like $A$, $B$, and the rest --- sentences that have
nothing to do with $M$. Yet the conclusion follows immediately by I.
This is as it should be: The truth conditions for the disjunction mean
that, if is true, then $\metaA{}\eor \metaB{}$ is true regardless of
what is. So the conclusion could not be false if the premise were true;
the argument is valid.

Now consider the disjunction elimination rule. What can you conclude
from $M \eor N$? You cannot conclude $M$. It might be $M$'s truth that
makes $M \eor N$ true, as in the example above, but it might not. From
$M \eor N$ alone, you cannot conclude anything about either $M$ or $N$
specifically. If you also knew that $N$ was false, however, then you
would be able to conclude $M$.

This is the disjunctive syllogism rule mentioned in the introduction. It
will be our official disjunction elimination rule (E). If you have a
disjunction and also the negation of one of its disjuncts, you may
conclude the other disjunct.

2

œab,nb

œab,nb

We represent two different inference patterns here, because the rule
allows you to conclude *either* disjunct from the negation of the other.
If we'd only listed the left version of the rule above, then E would've
only permited one to conclude the *first* disjunct from the negation of
the *second* one, along with the disjunction. Our rule lets us work with
either disjunction. (If you want to be very fussy about it, you could
think of these as two different rules with a strong conceptual
similarity that happen to have the same name.)

### Conditional

Consider this argument:

$R \eor F$

$\enot R \eif F$

The argument seems like it should be valid. (You can confirm this by
examining the truth tables.) What should the conditional introduction
rule be, such that we can draw this conclusion?

We begin the proof by writing down the premise of the argument and
drawing a horizontal line, like this:

If we had $\enot R$ as a further premise, we could derive $F$ by the E
rule. We do not have $\enot R$ as a premise of this argument, nor can we
derive it directly from the premise we do have --- so we cannot simply
prove $F$. What we will do instead is start a *subproof*, a proof within
the main proof. When we start a subproof, we draw another vertical line
to indicate that we are no longer in the main proof. Then we write in an
assumption for the subproof. This can be anything we want. Here, it will
be helpful to assume $\enot R$. Our proof now looks like this:

It is important to notice that we are not claiming to have proven
$\enot R$. We do not need to write in any justification for the
assumption line of a subproof. The vertical line indicates that an
*assumption* is being made. You can think of the subproof as posing the
question: What could we show *if* $\enot R$ were true? For one thing, we
can derive $F$. So we do:

œrf, nr

This has shown that *if* we had $\enot R$ as a premise, *then* we could
prove $F$. In effect, we have proven $\enot R \eif F$. So the
conditional introduction rule (I) will allow us to close the subproof
and derive $\enot R \eif F$ in the main proof. Our final proof looks
like this:

œrf, nr

The I lets us the assumption we'd been making, ending that vertical
line. During lines (2) and (3), we were *assuming* that $R$; by the time
we get to line (4), we are no longer making that assumption.

Notice that the justification for applying the I rule is the entire
subproof. That's why we justify it by reference to a range of lines,
instead of a comma-separated list. Usually that will be more than just
two lines.

It may seem as if the ability to assume anything at all in a subproof
would lead to chaos: Does it allow you to prove any conclusion from any
premises? The answer is no, it does not. Consider this proof:

Does this show that one can prove any arbitrary sentence from any
arbitrary premise ? After all, we've written on a line of a proof that
began with , without violating any of the rules of our system. The
reason this doesn't have that implication is the vertical line that
still extends into line 3. That line indicates that the assumption made
at line 2 is still in effect. When the vertical line for the subproof
ends, the subproof is *closed*. In order to complete a proof, you must
close all of the subproofs. The conclusion to be proved must not be
'blocked off' by a vertical line; it should be aligned with the
premises.

In this example, there is no way to close the subproof and use the R
rule again on line 4 to derive in the main proof. Once you close a
subproof, you cannot refer back to individual lines inside it. One can
only close a subproof via particular rules that allow you to do so; I is
one such rule. One can't just close a subproof willy-nilly. Closing a
subproof is called *discharging* the assumptions of that subproof. So we
can put the point this way: You cannot complete a proof until you have
discharged all of the assumptions besides the original premises of the
argument.

Of course, it is legitimate to do this:

This should not seem so strange, though. Since is a tautology, no
particular premises should be required to validly derive it. (Indeed, as
we will see, a tautology follows from any premises.)

Put in its general form, the I rule looks like this:

When we introduce a subproof, we typically write what we want to derive
off to the right. This is just so that we do not forget why we started
the subproof if it goes on for five or ten lines. There is no 'want'
rule. It is a note to ourselves, and not formally part of the proof.

Although it is always permissible to open a subproof with any assumption
you please, there is some strategy involved in picking a useful
assumption. Starting a subproof with an arbitrary, wacky assumption is
not a good strategy. It will just waste lines of the proof. In order to
derive a conditional by the I rule, for instance, you must assume the
antecedent of the conditional in a subproof.

The I rule also requires that the consequent of the conditional be the
last line of the subproof. It is always permissible to close a subproof
and discharge its assumptions, but it will not be helpful to do so until
you get what you want. This is an illustration of the observation made
above, that unlike the tree method, the natural deduction method
requires some strategy and thinking ahead.

Now consider the conditional elimination rule. Nothing follows from
$M\eif N$ alone, but if we have both $M \eif N$ and $M$, then we can
conclude $N$. This rule, modus ponens, will be the conditional
elimination rule (E).

Now that we have rules for the conditional, consider this argument:
[\[proofHS\]]{#proofHS label="proofHS"}

$P \eif Q$

$Q \eif R$

$P \eif R$

We begin the proof by writing the two premises as assumptions. Since the
main logical operator in the conclusion is a conditional, we can expect
to use the I rule. For that, we need a subproof --- so we write in the
antecedent of the conditional as assumption of a subproof:

We made $P$ available by assuming it in a subproof, allowing us to use E
on the first premise. This gives us $Q$, which allows us to use E on the
second premise. Having derived $R$, we close the subproof. By assuming
$P$ we were able to prove $R$, so we apply the I rule and finish the
proof.

[\[HSproof\]]{#HSproof label="HSproof"}

### Biconditional

Biconditionals indicate that the two sides have the same truth value.
One establishes a biconditional by establishing each direction of it as
conditionals. To derive $W \eiff X$, for instance, you must establish
both $W \eif X$ *and* $X \eif W$. Those conditionals may occur in either
order; they need not be on consecutive lines. (Compare the shape of the
I rule.) Schematically, the rule works like this:

The biconditional elimination rule (E) is a generalized version of
*modus ponens* (E). If you have the left-hand subsentence of the
biconditional, you can derive the right-hand subsentence. If you have
the right-hand subsentence, you can derive the left-hand subsentence.
This is the rule:

2

### Negation

Here is a simple mathematical argument in English:

Assume there is some greatest natural number. Call it $A$.

That number plus one is also a natural number.

Obviously, $A+1 > A$.

So there is a natural number greater than $A$.

This is impossible, since $A$ is assumed to be the greatest natural
number.

There is no greatest natural number.

This argument form is traditionally called a *reductio*. Its full Latin
name is *reductio ad absurdum*, which means 'reduction to absurdity.' In
a reductio, we assume something for the sake of argument --- for
example, that there is a greatest natural number. Then we show that the
assumption leads to two contradictory sentences --- for example, that
$A$ is the greatest natural number and that it is not. In this way, we
show that the original assumption must have been false.

The basic rules for negation will allow for arguments like this. If we
assume something and show that it leads to contradictory sentences, then
we have proven the negation of the assumption. This is the negation
introduction (I) rule:

\[ \]

The I rule discharges the assumption for reductio, concluding its
negation, when it's shown that some sentence and its negation each
follow from the assumption. It cites two (overlapping) ranges: a
subproof from the assumption to some sentence , and a subproof from that
same assumption to . We write 'for reductio' to the right of the
assumption, as a note to ourselves, a reminder of why we started the
subproof. It is not formally part of the proof, but it is helpful for
thinking clearly about the proof.

To see how the rule works, suppose we want to prove an instance of the
law of non-contradiction: $\enot(G \eand \enot G)$. We can prove this
without any premises by immediately starting a subproof. We want to
apply I to the subproof, so we assume $(G \eand \enot G)$. We then get
an explicit contradiction by E. The proof looks like this:

ægng ægng

The E rule will work in much the same way. If we assume and show that it
leads to a sentence and its negation, we have effectively proven . So
the rule looks like this:

\[ \]

Derived rules
-------------

The rules of the natural deduction system are meant to be systematic.
There is an introduction and an elimination rule for each logical
operator, but why these basic rules rather than some others? Some
natural deduction systems have a disjunction elimination rule that works
like this:

Let's call this rule Dilemma (DIL). It might seem as if there will be
some proofs that we cannot do with our proof system, because we do not
have this as a basic rule. Yet this is not the case. Any proof that you
can do using the Dilemma rule can be done with basic rules of our
natural deduction system. Consider a proof of this form:

œab, na

Remember once again that , , and are meta-variables. They are not
symbols of SL, but stand-ins for arbitrary sentences of SL. So this is
not, strictly speaking, a proof in SL. It is more like a recipe. It
provides a pattern that can prove anything that the Dilemma rule can
prove, using only the basic rules of SL. This means that the Dilemma
rule is not really necessary. Adding it to the list of basic rules would
not allow us to derive anything that we could not derive without it.

Nevertheless, the Dilemma rule would be convenient. It would allow us to
do in one line what requires eleven lines and several nested subproofs
with the basic rules. So we will add it to the proof system as a derived
rule.

A is a rule of proof that does not make any new proofs possible.
Anything that can be proven with a derived rule can be proven without
it. You can think of a short proof using a derived rule as shorthand for
a longer proof that uses only the basic rules. Anytime you use the
Dilemma rule, you could always take ten extra lines and prove the same
thing without it.

For the sake of convenience, we will add several other derived rules.
One is *modus tollens* (MT).

We leave the proof of this rule as an exercise. Note that if we had
already proven the MT rule, then the proof of the DIL rule could have
been done in only five lines.

We also add hypothetical syllogism (HS) as a derived rule. We have
already given a proof of it on p. .

Rules of replacement
--------------------

Consider how you would prove this argument valid: $F\eif(G\eand H)$
 $F\eif G$

Perhaps it is tempting to write down the premise and apply the E rule to
the conjunction $(G \eand H)$. This is impermissible, however, because
the basic rules of proof can only be applied to whole sentences. In
order to use E, we need to get the conjunction $(G \eand H)$ on a line
by itself. Here is a proof:

ægh

The rules we have seen so far must apply to wffs that are on a proof
line by themselves. We will now introduce some derived rules that may be
applied to part of a sentence. These are called , because they can be
used to replace part of a sentence with a logically equivalent
expression. One simple rule of replacement is commutativity (abbreviated
Comm), which says that we can swap the order of conjuncts in a
conjunction or the order of disjuncts in a disjunction. We define the
rule this way:

  ----------------------------------------------------------------------- ------
    $(\metaA{}\eand\metaB{}) \Longleftrightarrow (\metaB{}\eand\metaA{})$ 
      $(\metaA{}\eor\metaB{}) \Longleftrightarrow (\metaB{}\eor\metaA{})$ 
    $(\metaA{}\eiff\metaB{}) \Longleftrightarrow (\metaB{}\eiff\metaA{})$ Comm
  ----------------------------------------------------------------------- ------

The double arrow means that you can take a subformula on one side of the
arrow and replace it with the subformula on the other side. The arrow is
double-headed because rules of replacement work in both directions.

Consider this argument: $(M \eor P) \eif (P \eand M)$,
 $(P \eor M) \eif (M \eand P)$

It is possible to give a proof of this using only the basic rules, but
it will be long and inconvenient. With the Comm rule, we can provide a
proof easily:

Another rule of replacement is double negation (DN). With the DN rule,
you can remove or insert a pair of negations for any wff in a line, even
if it isn't the whole line. This is the rule:

  --------------------------------------------------- ----
    $\enot\enot\metaA{} \Longleftrightarrow \metaA{}$ DN
  --------------------------------------------------- ----

Two more replacement rules are called De Morgan's Laws, named for the
19th-century British logician August De Morgan. (Although De Morgan did
formalize and publish these laws, many others discussed them before
him.) The rules capture useful relations between negation, conjunction,
and disjunction. Here are the rules, which we abbreviate DeM:

  ------------------------------------------------------------------------------------- -----
    $\enot(\metaA{}\eor\metaB{}) \Longleftrightarrow (\enot\metaA{}\eand\enot\metaB{})$ 
    $\enot(\metaA{}\eand\metaB{}) \Longleftrightarrow (\enot\metaA{}\eor\enot\metaB{})$ DeM
  ------------------------------------------------------------------------------------- -----

As we have seen, $\metaA{}\eif\metaB{}$ is equivalent to
$\enot\metaA{}\eor\metaB{}$. A further replacement rule captures this
equivalence. We abbreviate the rule MC, for 'material conditional.' It
takes two forms:

  -------------------------------------------------------------------------- ----
    $(\metaA{}\eif\metaB{}) \Longleftrightarrow (\enot\metaA{}\eor\metaB{})$ 
    $(\metaA{}\eor\metaB{}) \Longleftrightarrow (\enot\metaA{}\eif\metaB{})$ MC
  -------------------------------------------------------------------------- ----

Now consider this argument: $\enot(P \eif Q)$,  $P \eand \enot Q$

As always, we could prove this argument valid using only the basic
rules. With rules of replacement, though, the proof is much simpler:

A final replacement rule captures the relation between conditionals and
biconditionals. We will call this rule biconditional exchange and
abbreviate it ex.

  --------------------------------------------------------------------------------------------------- ----
    $[(\metaA{}\eif\metaB{})\eand(\metaB{}\eif\metaA{})] \Longleftrightarrow (\metaA{}\eiff\metaB{})$ ex
  --------------------------------------------------------------------------------------------------- ----

Proof strategy {#sec.SL.ND.strategy}
--------------

There is no simple recipe for proofs, and there is no substitute for
practice. Here, though, are some rules of thumb and strategies to keep
in mind.

##### Work backwards from what you want.

The ultimate goal is to derive the conclusion. Look at the conclusion
and ask what the introduction rule is for its main logical operator.
This gives you an idea of what should happen *just before* the last line
of the proof. Then you can treat this line as if it were your goal. Ask
what you could do to derive this new goal.

For example: If your conclusion is a conditional $\metaA{}\eif\metaB{}$,
plan to use the I rule. This requires starting a subproof in which you
assume . In the subproof, you want to derive .

##### Work forwards from what you have.

When you are starting a proof, look at the premises; later, look at the
sentences that you have derived so far. Think about the elimination
rules for the main operators of these sentences. These will tell you
what your options are.

For example: If you have a conditional , and you also have , E is a
pretty natural choice.

For a short proof, you might be able to eliminate the premises and
introduce the conclusion. A long proof is formally just a number of
short proofs linked together, so you can fill the gap by alternately
working back from the conclusion and forward from the premises.

##### Change what you are looking at.

Replacement rules can often make your life easier. If a proof seems
impossible, try out some different substitutions.

For example: It is often difficult to prove a disjunction using the
basic rules. If you want to show $\metaA{}\eor\metaB{}$, it is often
easier to show $\enot\metaA{}\eif\metaB{}$ and use the MC rule.

Some replacement rules should become second nature. If you see a negated
disjunction, for instance, you should immediately think of DeMorgan's
rule.

##### Do not forget indirect proof.

If you cannot find a way to show something directly, try assuming its
negation.

Remember that most proofs can be done either indirectly or directly. One
way might be easier --- or perhaps one sparks your imagination more than
the other --- but either one is formally legitimate.

##### Repeat as necessary.

Once you have decided how you might be able to get to the conclusion,
ask what you might be able to do with the premises. Then consider the
target sentences again and ask how you might reach them.

##### Persist.

Try different things. If one approach fails, then try something else.

Proof-theoretic concepts
------------------------

As we did in our discussion of trees, we will again use the symbol
'$\vdash$' to indicate provability. Provability is relative to a proof
system, so the meaning of the '$\vdash$' symbol featured in this chapter
should be distinguished from the one we used for trees. When necessary,
we can specify the single turnstile with reference to the proof system
in question, letting '$\vdash_{T}$' stand for provability in the tree
system, and '$\vdash_{ND}$' stand for provability in this natural
deduction system. For the most part in this chapter, though, we'll be
interested in natural deduction, so unless it is specified otherwise,
you can understand '$\vdash$' to mean '$\vdash_{ND}$'.

The double turnstile symbol '$\models$', remains unchanged. It stands
for semantic entailment, as described in
ch. [\[ch.SLmodels\]](#ch.SLmodels){reference-type="ref"
reference="ch.SLmodels"}.

When we write $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash_{ND}\metaB{}$,
this means that it is possible to give a natural deduction proof of with
$\metaA{}_1$,$\metaA{}_2$,$\ldots$ as premises. With just one premise,
we leave out the curly braces, so $\metaA{}\vdash\metaB{}$ means that
there is a proof of with as a premise. Naturally, $\vdash\metaA{}$ means
that there is a proof of that has no premises. You can think of it as
shorthand for $\emptyset\vdash\metaA{}$.

For notational completeness, we can understand $\metaSetX{}\vdash\bot$
to mean that from , we could prove an arbitrary contradiction. In other
words, $\metaSetX{}$ is an inconsistent set.

Logical proofs are sometimes called *derivations*. So
$\metaA{}\vdash\metaB{}$ can be read as ' is derivable from .'

A is a sentence that is derivable without any premises; i.e., is a
theorem if and only if $\vdash\metaA{}$.

It is not too hard to show that something is a theorem --- you just have
to give a proof of it. How could you show that something is *not* a
theorem? If its negation is a theorem, then you could provide a proof.
For example, it is easy to prove $\enot(P \eand \enot P)$, which shows
that $(P \eand \enot P)$ cannot be a theorem. For a sentence that is
neither a theorem nor the negation of a theorem, however, there is no
easy way to show this. You would have to demonstrate not just that
certain proof strategies fail, but that no proof is possible. Even if
you fail in trying to prove a sentence in a thousand different ways,
perhaps the proof is just too long and complex for you to make out. As
we've emphasized already, this is a difference between our natural
deduction system and the tree method.

Two sentences and are if and only if each can be derived from the other;
i.e., $\metaA{}\vdash\metaB{}$ and $\metaB{}\vdash\metaA{}$.

It is relatively easy to show that two sentences are provably equivalent
--- it just requires a pair of proofs. Showing that sentences are *not*
provably equivalent would be much harder. It would be just as hard as
showing that a sentence is not a theorem. (In fact, these problems are
interchangeable. Can you think of a sentence that would be a theorem if
and only if and were provably equivalent?)

The set of sentences $\{\metaA{}_1,\metaA{}_2,\ldots\}$ is if and only
if contradictory sentences are derivable from it; i.e., for some
sentence , $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash\metaB{}$ and
$\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash\enot \metaB{}$. This is
equivalent to $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash\bot$.

It is easy to show that a set is provably inconsistent: You just need to
assume the sentences in the set and prove a contradiction. Showing that
a set is *not* provably inconsistent will be much harder. It would
require more than just providing a proof or two; it would require
showing that proofs of a certain kind are *impossible*.

Proofs and models
-----------------

As you might already suspect, there is a connection between *theorems*
and *tautologies*.

There is a formal way of showing that a sentence is a theorem: Prove it.
For each line, we can check to see if that line follows by the cited
rule. It may be hard to produce a twenty line proof, but it is not so
hard to check each line of the proof and confirm that it is legitimate
--- and if each line of the proof individually is legitimate, then the
whole proof is legitimate. Showing that a sentence is a tautology,
though, requires reasoning in English about all possible models. There
is no formal way of checking to see if the reasoning is sound. Given a
choice between showing that a sentence is a theorem and showing that it
is a tautology, it would be easier to show that it is a theorem.

By contrast, there is no formal way of showing that a sentence is *not*
a theorem. We would need to reason in English about all possible proofs.
Yet there is a formal method for showing that a sentence is not a
tautology. We need only construct a model in which the sentence is
false. Given a choice between showing that a sentence is not a theorem
and showing that it is not a tautology, it would be easier to show that
it is not a tautology.

Fortunately, a sentence is a theorem if and only if it is a tautology.
If we provide a proof of $\vdash\metaA{}$ and thus show that it is a
theorem, it follows that is a tautology; i.e., $\models\metaA{}$.
Similarly, if we construct a model in which is false and thus show that
it is not a tautology, it follows that is not a theorem.

In general, $\metaA{}\vdash\metaB{}$ if and only if
$\metaA{}\models\metaB{}$. As such:

-   An argument is *valid* if and only if *the conclusion is derivable
    from the premises*.

-   Two sentences are *logically equivalent* if and only if they are
    *provably equivalent*.

-   A set of sentences is *consistent* if and only if it is *not
    provably inconsistent*.

You can pick and choose when to think in terms of proofs and when to
think in terms of models, doing whichever is easier for a given task.
Table [1.1](#table.ProofOrModel){reference-type="ref"
reference="table.ProofOrModel"} summarizes when it is best to give
proofs and when it is best to give models.

In this way, proofs and models give us a versatile toolkit for working
with arguments. If we can translate an argument into SL, then we can
measure its logical weight in a purely formal way. If it is deductively
valid, we can give a formal proof; if it is invalid, we can provide a
formal counterexample.

::: {#table.ProofOrModel}
  ----------------------------------- ------------------------------------------------------------- ---------------------------------------------------------------
                                                                                                    
  Is a tautology?                     prove $\vdash\metaA{}$                                        give a model in which is false
  Is a contradiction?                 prove $\vdash\enot\metaA{}$                                   give a model in which is true
  Is contingent?                      give a model in which is true and another in which is false   prove $\vdash\metaA{}$ or $\vdash\enot\metaA{}$
  Are and equivalent?                 prove $\metaA{}\vdash\metaB{}$ and $\metaB{}\vdash\metaA{}$   give a model in which and have different truth values
  Is the set consistent?              give a model in which all the sentences in are true           taking the sentences in , prove and
  Is the argument ', , ... ' valid?   prove $\metaA{}, \metaB{}, \ldots \vdash\metaC{}$             give a model in which {, , ...} is satisfied and is falsified
                                                                                                    
  ----------------------------------- ------------------------------------------------------------- ---------------------------------------------------------------

  : Sometimes it is easier to show something by providing proofs than it
  is by providing models. Sometimes it is the other way round. It
  depends on what you are trying to show.
:::

Soundness and completeness
--------------------------

Chapter
[\[ch.SLsoundcomplete\]](#ch.SLsoundcomplete){reference-type="ref"
reference="ch.SLsoundcomplete"} considered the soundness and
completeness of the tree method at length; it proved that this method
was both sound ($\metaSetX{}\vdash_{T}\metaA{}$ only if
$\metaSetX{}\models{}$) and complete ($\metaSetX{}\models{}$ only if
$\metaSetX{}\vdash_{T}\metaA{}$). The natural deduction system of this
chapter is also both sound and complete for SL. In other words,
$\metaSetX{}\vdash_{ND}\metaA{}$ if and only if
$\metaSetX{}\models{}\metaA{}.$ Given the soundness and completeness of
the tree method, this also means that our two proof systems are
equivalent in the sense that anything provable in one is also provable
in the other ($\metaSetX{}\vdash_{T}\metaA{})$ iff
($\metaSetX{}\vdash_{ND}\metaA{}$).

How can we know that our natural deduction method is sound? A proof
system is if there are no derivations corresponding to invalid
arguments. Demonstrating that the proof system is sound would require
showing that any possible proof in our system is the proof of a valid
argument. There is a fairly simple way of approaching this in a
step-wise fashion. If using the E rule on the last line of a proof could
never change a valid argument into an invalid one, then using the rule
many times could not make an argument invalid. Similarly, if using the E
and E rules individually on the last line of a proof could never change
a valid argument into an invalid one, then using them in combination
could not either.

The strategy is to show for every rule of inference that it alone could
not make a valid argument into an invalid one. It follows that the rules
used in combination would not make a valid argument invalid. Since a
proof is just a series of lines, each justified by a rule of inference,
this would show that every provable argument is valid.

Consider, for example, the I rule. Suppose we use it to add to a valid
argument. In order for the rule to apply, and must already be available
in the proof. Since the argument so far is valid, and are either
premises of the argument or valid consequences of the premises. As such,
any model in which the premises are true must be a model in which and
are true. According to the definition of , this means that is also true
in such a model. Therefore, validly follows from the premises. This
means that using the I rule to extend a valid proof produces another
valid proof.

In order to show that the proof system is sound, we would need to show
this for the other inference rules. Since the derived rules are
consequences of the basic rules, it would suffice to provide similar
arguments for the 16 other basic rules. The reasoning is extremely
similar to that given in the soundness proof for trees in the previous
chapter. We will not go through it in detail here.

Given a proof that the proof system is sound, it follows that every
theorem is a tautology.

What of completeness? Why think that *every* valid argument is an
argument that can be proven in our natural deduction system? That is,
why think that $\metaA{}\models\metaB{}$ implies
$\metaA{}\vdash\metaB{}$? Our system *is* also complete, but the
completeness proof for natural deduction is a bit more complex than the
completeness proof for trees. (In the case of trees, we had a mechanical
method that was guaranteed to find proofs if they exist; we have seen no
such method here, which makes proving these general results harder.)
This proof is beyond the scope of this book.

The important point is that, happily, the proof system for SL is both
sound and complete. Consequently, we may freely use this natural
deduction method to draw conclusions about models in SL.

Summary of definitions {#summary-of-definitions .unnumbered}
----------------------

-   A sentence is a if and only if $\vdash\metaA{}$.

-   Two sentences and are if and only if $\metaA{}\vdash\metaB{}$ and
    $\metaB{}\vdash\metaA{}$.

-   $\{\metaA{}_1,\metaA{}_2,\ldots\}$ is if and only if, for some
    sentence ,
    $\{\metaA{}_1,\metaA{}_2,\ldots\}\vdash(\metaB{} \eand \enot \metaB{})$.

[\[pr.justifySLproof\]]{#pr.justifySLproof label="pr.justifySLproof"}
Provide a justification (rule and line numbers) for each line of proof
that requires one.

2

[\[pr.solvedSLproofs\]]{#pr.solvedSLproofs label="pr.solvedSLproofs"}
Give a proof for each argument in SL.

$K\eand L$, $K\eiff L$

$A\eif (B\eif C)$, $(A\eand B)\eif C$

$P \eand (Q\eor R)$, $P\eif \enot R$, $Q\eor E$

$(C\eand D)\eor E$, $E\eor D$

$\enot F\eif G$, $F\eif H$, $G\eor H$

$(X\eand Y)\eor(X\eand Z)$, $\enot(X\eand D)$, $D\eor M$ $M$

Give a proof for each argument in SL.

$Q\eif(Q\eand\enot Q)$,  $\enot Q$

$J\eif\enot J$,  $\enot J$

$E\eor F$, $F\eor G$, $\enot F$,  $E \eand G$

$A\eiff B$, $B\eiff C$,  $A\eiff C$

$M\eor(N\eif M)$,  $\enot M \eif \enot N$

$S\eiff T$,  $S\eiff (T\eor S)$

$(M \eor N) \eand (O \eor P)$, $N \eif P$, $\enot P$,  $M\eand O$

$(Z\eand K) \eor (K\eand M)$, $K \eif D$,  $D$

[\[pr.SLND.theorems\]]{#pr.SLND.theorems label="pr.SLND.theorems"} Show
that each of the following sentences is a theorem in SL.

$O \eif O$

$N \eor \enot N$

$\enot(P\eand \enot P)$

$\enot(A \eif \enot C) \eif (A \eif C)$

$J \eiff [J\eor (L\eand\enot L)]$

Show that each of the following pairs of sentences are provably
equivalent in SL.

$\enot\enot\enot\enot G$, $G$

$T\eif S$, $\enot S \eif \enot T$

$R \eiff E$, $E \eiff R$

$\enot G \eiff H$, $\enot(G \eiff H)$

$U \eif I$, $\enot(U \eand \enot I)$

[\[pr.solvedSLproofs2\]]{#pr.solvedSLproofs2 label="pr.solvedSLproofs2"}
Provide proofs to show each of the following.

$M \eand (\enot N \eif \enot M) \vdash (N \eand M) \eor \enot M$

{$C\eif(E\eand G)$, $\enot C \eif G$} $\vdash$ $G$

{$(Z\eand K)\eiff(Y\eand M)$, $D\eand(D\eif M)$} $\vdash$ $Y\eif Z$

{$(W \eor X) \eor (Y \eor Z)$, $X\eif Y$, $\enot Z$} $\vdash$ $W\eor Y$

For the following, provide proofs using only the basic rules. The proofs
will be longer than proofs of the same claims would be using the derived
rules.

Show that MT is a legitimate derived rule. Using only the basic rules,
prove the following: , ,  

Show that Comm is a legitimate rule for the biconditional. Using only
the basic rules, prove that $\metaA{}\eiff\metaB{}$ and
$\metaB{}\eiff\metaA{}$ are equivalent.

Using only the basic rules, prove the following instance of DeMorgan's
Laws: $(\enot A \eand \enot B)$,  $\enot(A \eor B)$

Show that ex is a legitimate derived rule. Using only the basic rules,
prove that $D\eiff E$ and $(D\eif E)\eand(E\eif D)$ are equivalent.

If you know that $\metaA{}\vdash\metaB{}$, what can you say about
$(\metaA{}\eand\metaC{})\vdash\metaB{}$? Explain your answer.

If you know that $\metaA{}\vdash\metaB{}$, what can you say about
$(\metaA{}\eor\metaC{})\vdash\metaB{}$? Explain your answer.
